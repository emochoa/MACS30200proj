---
title: "MACS 30200 PS3"
author: "Erin M. Ochoa"

date: "2017 May 15"
output:
  github_document:
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)

library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(gridExtra)
library(stringr)
library(ISLR)
library(haven)
library(extrafont)
library(SparseM)
library(dplyr)
library(car)
library(lmtest)
library(GGally)
library(nortest)
library(MVN)
library(Amelia)

options(digits = 3)
set.seed(1337)
theme_set(theme_minimal())
```

# Regression diagnostics


Estimate the following linear regression model of attitudes towards Joseph Biden:

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3$$
where $Y$ is the Joe Biden feeling thermometer, $X_1$ is age, $X_2$ is gender, and $X_3$ is education. Report the parameters and standard errors.

```{r read_data_fit_model}

df = read.csv('biden.csv')

df$Gender = factor(df$female, labels=c('Man','Woman'))

df$Party[df$dem == 1] = 'Democrat'
df$Party[df$dem == 0 & df$rep == 0] = 'No Affiliation'
df$Party[df$rep == 1] = 'Republican'

df = na.omit(df)

lm_biden = lm(biden ~ age + Gender + educ, df)

summary(lm_biden)

biden_augment = df %>%
                mutate(hat = hatvalues(lm_biden),
                student = rstudent(lm_biden),
                cooksd = cooks.distance(lm_biden))

```

The y-intercept has a coefficient of `r summary(lm_biden)$coefficients[1,1]` and a standard error of `r summary(lm_biden)$coefficients[1,2]`; it is statistically significant at the p<.001 level.  $X_1$ (age) has a coefficient of `r summary(lm_biden)$coefficients[2,1]` and a standard error of `r summary(lm_biden)$coefficients[2,2]`; it is not statistically significant (p=`r summary(lm_biden)$coefficients[2,4]`).  $X_2$ (female) has a coefficient of `r summary(lm_biden)$coefficients[3,1]` and a standard error of `r summary(lm_biden)$coefficients[3,2]`; it is statistically significant at the p<.001 level.  $X_3$ (education) has a coefficient of `r summary(lm_biden)$coefficients[4,1]` and a standard error of `r summary(lm_biden)$coefficients[4,2]`; it is also statistically significant at the p<.001 level.

## Testing for unsual and/or influential observations

### High-leverage observations

First, we plot high-leverage observations (those with a leverage value greater than twice the average leverage value):

``` {r high-lev_obs, echo=FALSE}

biden_lev = biden_augment %>%
            filter(hat > 2 * mean(hat))

ggplot(biden_lev, mapping = aes(x = age, y = hat)) +
       geom_point(alpha=.8, aes(shape = Gender, color = biden, size = educ)) + 
       scale_color_continuous(name = "Biden Feeling Thermometer") +
       scale_size_continuous(name = "Years of Education") +
       labs(title = "Biden Feeling Thermometer by Age, Gender, & Education:\nHigh-Leverage Observations",
            subtitle = 'N = 74',
            x = "Respondent Age",
            y = "Hat Value") +
       theme(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

We can see that most of the high-leverage observations are of higher feeling-thermometer values from older respondents of both genders and with more years of schooling.  The two points with the highest leverage values, however, are from respodents with lower levels of education.

### Discrepant residuals

Next, we plot discrepant residuals (those with an absolute value greater than 2):

``` {r student_resids, echo=FALSE}
biden_resid = biden_augment %>%
              filter(abs(student) > 2)

ggplot(biden_resid, mapping = aes(x = age, y = abs(student))) +
       geom_point(alpha=.8, aes(shape = Gender, color = biden, size = educ)) + 
       scale_color_continuous(name = "Biden Feeling Thermometer") +
       scale_size_continuous(name = "Years of Education") +
       labs(title = "Biden Feeling Thermometer by Age, Gender, & Education:\nDiscrepant Residuals",
            subtitle = 'N = 82',
            x = "Respondent Age",
            y = "Studentized Residual Absolute Value") +
       theme(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

We see that most discrepant residuals are from respondents with higher levels of education, but that they vary in age and gender.  Interestingly, those with lower absolute residual values (relative to the discrepant residuals) tend to be women with higher scores for Biden warmth, while those with average and high residual values (again, relative to the discrepant residuals) tend to be men and women, respectively, with low Biden warmth scores.

### Influential observations

We now plot influential observations (those with high Cook's D values):

``` {r influential_observations, echo=FALSE}
biden_infl = biden_augment %>%
             filter(cooksd > 4 / (nrow(.) - (length(coef(lm_biden)) - 1) - 1))

ggplot(biden_infl, mapping = aes(x = age, y = cooksd)) +
       geom_point(alpha=.7, aes(shape = Gender, color = biden, size = educ)) + 
       scale_color_continuous(name = "Biden Feeling Thermometer") +
       scale_size_continuous(name = "Years of Education") +
       labs(title = "Biden Feeling Thermometer by Age, Gender, & Education:\nHigh-Influence Observations",
            subtitle = 'N = 90',
            x = "Respondent Age",
            y = "Cook's D") +
       theme(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

We can see that the respondents vary by age and gender, but that most have low values for Biden warmth and and moderate to high education.

We plot all the unusual observations together, with residuals versus leverage and colored by Cook's D:

``` {r bubble_plot, echo=FALSE}

biden_outliers = biden_augment %>%
                 filter(cooksd > 4 / (nrow(.) - (length(coef(lm_biden)) - 1) - 1) | abs(student) > 2 | hat > 2 * mean(hat))

ggplot(biden_outliers, aes(hat, student)) +
       geom_hline(yintercept = 0, linetype = 2) +
       geom_point(aes(color = cooksd), size = 2.5, alpha = .7, position='jitter') +
       scale_color_continuous(name="Cook's D") +
       labs(title = 'Biden Feeling Thermometer by Age, Gender, & Education:\nUnusual Observations',
            subtitle = 'N = 167',
            x = "Leverage",
            y = "Studentized Residual") +
       theme(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

We see that the observations with mid-level Cook's D values are concentrated in the lower left corner of the plot; these points have low leverage and negative studentized residuals of high distance.

Next, we create histograms of the outliers compared to all the observations:

```{r outliers_age_gender_party, echo=FALSE}

plotA = ggplot(df, aes(x=age, fill=Gender)) +
               geom_histogram(binwidth = 5, position = 'dodge', aes(y = ..count../sum(..count..))) +
               ylab("Proportion of respondents") +
               ggtitle("Observations by Respondent Age") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     legend.position = "none",
                     axis.title.x=element_blank(),
                     axis.text.x=element_blank(),
                     axis.ticks.x=element_blank(),
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.125), breaks = seq(0, .125, by=.025))


plotB = ggplot(biden_outliers, aes(x=age, fill=Gender)) +
               geom_histogram(binwidth = 5, position = 'dodge', aes(y = ..count../sum(..count..))) +
               ggtitle("Outliers by Respondent Age") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     axis.title.x=element_blank(),
                     axis.text.x=element_blank(),
                     axis.ticks.x=element_blank(),
                     axis.title.y=element_blank(),
                     axis.text.y=element_blank(),
                     axis.ticks.y=element_blank(),
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.125), breaks = seq(0, .125, by=.025)) + 
               scale_fill_discrete(labels=c('M','W'))

plotC = ggplot(df, aes(x=age, fill=Party)) +
               geom_histogram(binwidth = 5, position = 'dodge', aes(y = ..count../sum(..count..))) +
               ylab("Proportion of respondents") +
               xlab("Respondent Age") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     legend.position = "none",
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.1), breaks = seq(0, .1, by=.025)) +
               scale_fill_manual(values=c('deeppink','darkturquoise','orange'))


plotD = ggplot(biden_outliers, aes(x=age, fill=Party)) +
               geom_histogram(binwidth = 5, position = 'dodge', aes(y = ..count../sum(..count..))) +
               xlab("Respondent Age") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     axis.title.y=element_blank(),
                     axis.text.y=element_blank(),
                     axis.ticks.y=element_blank(),
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.1), breaks = seq(0, .1, by=.025)) + 
               scale_fill_manual(name = 'Party ', labels=c('D','I','R'),values=c('deeppink','darkturquoise','orange'))

grid.arrange(plotA, plotB, plotC, plotD, ncol=2, nrow=2, widths=c(.48,.52))
```

We see that respondents of advanced age, particularly men, are overrepresented among the outliers.  Additionally, older respondents from all parties and middle-aged Republicans are overrepresented among the outliers.


```{r outliers_biden_gender_party, warning=FALSE, echo=FALSE}

plotE = ggplot(df, aes(x=biden, fill=Gender)) +
               geom_histogram(binwidth = 10, position = 'dodge', aes(y = ..count../sum(..count..))) +
               ylab("Proportion of respondents") +
               ggtitle("Observations by Respondent Age") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     legend.position = "none",
                     axis.title.x=element_blank(),
                     axis.text.x=element_blank(),
                     axis.ticks.x=element_blank(),
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.125), breaks = seq(0, .125, by=.025))


plotF = ggplot(biden_outliers, aes(x=biden, fill=Gender)) +
               geom_histogram(binwidth = 10, position = 'dodge', aes(y = ..count../sum(..count..))) +
               ggtitle("Outliers by Respondent Age") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     axis.title.x=element_blank(),
                     axis.text.x=element_blank(),
                     axis.ticks.x=element_blank(),
                     axis.title.y=element_blank(),
                     axis.text.y=element_blank(),
                     axis.ticks.y=element_blank(),
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.125), breaks = seq(0, .125, by=.025)) + 
               scale_fill_discrete(labels=c('M','W'))

plotG = ggplot(df, aes(x=biden, fill=Party)) +
               geom_histogram(binwidth = 10, position = 'dodge', aes(y = ..count../sum(..count..))) +
               ylab("Proportion of respondents") +
               xlab("Biden Warmth") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     legend.position = "none",
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.1), breaks = seq(0, .1, by=.025)) +
               scale_fill_manual(values=c('deeppink','darkturquoise','orange'))


plotH = ggplot(biden_outliers, aes(x=biden, fill=Party)) +
               geom_histogram(binwidth = 10, position = 'dodge', aes(y = ..count../sum(..count..))) +
               xlab("Biden Warmth") +
               theme(plot.title = element_text(hjust = 0.5),
                     panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
                     panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1),
                     axis.title.y=element_blank(),
                     axis.text.y=element_blank(),
                     axis.ticks.y=element_blank(),
                     legend.title = element_text(family='mono')) + 
               scale_y_continuous(limits = c(0,.1), breaks = seq(0, .1, by=.025)) + 
               scale_fill_manual(name = 'Party ', labels=c('D','I','R'),values=c('deeppink','darkturquoise','orange'))

grid.arrange(plotE, plotF, plotG, plotH, ncol=2, nrow=2, widths=c(.48,.52))
```

We see that women with very low warmth scores and men with very high warmth scores are overrepresented among the outliers, as are Independents and Republicans with very low warmth scores.

Next, we visualize the outliers alone, with Biden warmth versus respondent age, colored by party and shaped by gender:

``` {r scatter_plot, echo=FALSE}


ggplot(biden_outliers, aes(x=age, y=biden)) +
       geom_point(aes(color=Party, shape=Gender), size = 3, position = 'jitter', alpha = .65) + 
       xlab("Respondent Age") +
       ylab("Biden Warmth") + 
       ggtitle("Outliers by Respondent Age") +
       theme(plot.title = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1)) + 
               #scale_y_continuous(limits = c(0,.1), breaks = seq(0, .1, by=.025)) + 
       scale_color_manual(name = 'Party ', labels=c('D','I','R'),values=c('deeppink','darkturquoise','orange')) +
       scale_shape(labels=c('M','W'))
```

As indicated by the histograms above, we see that mast of the outliers with very high warmth scores are senior respondents of both genders who identify as Democrats.  Most of the outliers with low warmth scores identify as Republicans or Independents, span the full range of ages, and vary by gender.

### Next steps
If we were moving forward with this research, we would run the model without the unusual observations and compare the results with those generated from the model that uses all the observations.  We would also consider adding political party to the model because it seems to have an effect on Biden warmth scores, at least among outliers and potentially among the rest of the observations.  Finally, we would consider adding one or more interaction terms to the model because older Democrats and older men are overrepresented among the outliers; adding such interaction terms could help explain the relationship between those variables.


## Non-normally distributed errors

First, we generate a Q–Q plot:

``` {r qqplot, echo=FALSE}

car::qqPlot(lm_biden)
```

Many of the observations, particularly those in the S-shape at the tail ends of the distribution, fall outside the confidence bands.  This indicates that the errors are not normally distributed.

To confirm this, we generate a density plot of the studentized residuals:

``` {r density_plot, echo=FALSE}

augment(lm_biden, df) %>%
  mutate(.student = rstudent(lm_biden)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5, color='deeppink',fill='deeppink',alpha = .5) +
  labs(x = "Studentized residuals",
       y = "Estimated density") + 
  ggtitle("Density Plot of Studentized Residuals") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

The very heavy left skew and multiple peaks confirm that the residuals are not normally distributed.

### Next steps

To correct this, we could use a power or log transformation on one or more of the continuous variables (age and Biden warmth).  As with outliers above, we could also considering adding party and/or interaction terms to the model.


## Heteroscedasticity

We evaluate the plot of residuals versus predicted warmth in order to assess homoscedasticity:

``` {r homoscedasticity_plot, message=FALSE}

biden_augment %>%
  add_predictions(lm_biden) %>%
  add_residuals(lm_biden) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2, color='darkturquoise') +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95), color='deeppink', size=1) +
  labs(title = "Variance of Error Terms",
       x = "Predicted Warmth",
       y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

The confidence bands do not resemble horizontal lines, indicating that the errors have non-constant variance.  This is particularly true for residuals with low-to-medium predicted warmth.  We conduct a Breusch-Pagan test to formally assess homoscedasticity:

``` {r bp_test}

bptest(lm_biden)
```

The null hypothesis for the Breusch-Pagan test is that the errors have constant variance.  With a p-value < .001, we reject the null hypothesis and find that the errors do exhibit heteroscedasticity.

This has implications for inference because non-constant variance in the error terms can artificially augment or diminish the standard errors; this could then influence the t-statistics and accompanying p-values for different coefficients, perhaps leading us to falsely identify a variable as statistically significant when it is not, or, conversely, fail to identify a variable as statistically significant when it is indeed so.

## Multicollinearity

Finally, we test for multicollinarity with a correlation heatmap:

``` {r correlation_heatmap}
#Adapted slightly from code by bsoltoff
cormat_heatmap = function(data){
  # generate correlation matrix
  cormat = round(cor(data), 2)
  
  # melt into a tidy table
  get_upper_tri = function(cormat){
    cormat[lower.tri(cormat)] = NA
    return(cormat)
  }
  
  upper_tri = get_upper_tri(cormat)
  
  # reorder matrix based on coefficient value
  reorder_cormat = function(cormat){
    # Use correlation between variables as distance
    dd = as.dist((1-cormat)/2)
    hc = hclust(dd)
    cormat <-cormat[hc$order, hc$order]
  }
  
  cormat = reorder_cormat(cormat)
  upper_tri = get_upper_tri(cormat)
  
  # Melt the correlation matrix
  melted_cormat = reshape2::melt(upper_tri, na.rm = TRUE)
  
  # Create a ggheatmap
  ggheatmap = ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "darkturquoise", high = "deeppink", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  # add correlation values to graph
  ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.position = "bottom",
      plot.title = element_text(hjust = 0.5)) +
    labs(title='Correlation Heatmap')
}

cormat_heatmap(select_if(df, is.numeric))
```

While we see that warmth score and Democrat are moderately negatively correlated with Republican, and that Democrat is moderately correlated with warmth score, no two independent variables are moderately or strongly correlated with one another.  This indicates that multicollinearity is not a problem in this model.

We confirm this by calculating variance inflation factors (VIF) for the variables in the regression model:

``` {r vif}

vif(lm_biden)
```

The VIF statistics are all below 10, which confirms that multicollinearity is not a concern here.






# Interaction terms

Estimate the following linear regression model:

$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2$$

where $Y$ is the Joe Biden feeling thermometer, $X_1$ is age, and $X_2$ is education. Report the parameters and standard errors.


``` {r lm_interaction}
lm_int_biden = lm(biden ~ age + educ + age*educ, df)

summary(lm_int_biden)
```

The y-intercept has a coefficient of `r summary(lm_biden)$coefficients[1,1]` and a standard error of `r summary(lm_biden)$coefficients[1,2]`; it is statistically significant at the p<.001 level.  $X_1$ (age) has a coefficient of `r summary(lm_biden)$coefficients[2,1]` and a standard error of `r summary(lm_biden)$coefficients[2,2]`; it is statistically significant at the p<.001 level.  $X_2$ (education) has a coefficient of `r summary(lm_biden)$coefficients[3,1]` and a standard error of `r summary(lm_biden)$coefficients[3,2]`; it is statistically significant at the p<.05 level.  $X_1X_2$ has a coefficient of `r summary(lm_biden)$coefficients[4,1]` and a standard error of `r summary(lm_biden)$coefficients[4,2]`; it is statistically significant at the p<.001 level.

### Marginal effect of age on warmth, conditional on education

``` {r instant_effect}

#Code by bsoltoff
instant_effect = function(model, mod_var){
  # get interaction term name
  int.name = names(model$coefficients)[[which(str_detect(names(model$coefficients), ":"))]]
  
  marg_var = str_split(int.name, ":")[[1]][[which(str_split(int.name, ":")[[1]] != mod_var)]]
  
  # store coefficients and covariance matrix
  beta.hat = coef(model)
  cov = vcov(model)
  
  # possible set of values for mod_var
  if(class(model)[[1]] == "lm"){
    z <- seq(min(model$model[[mod_var]]), max(model$model[[mod_var]]))
  } else {
    z = seq(min(model$data[[mod_var]]), max(model$data[[mod_var]]))
  }
  
  # calculate instantaneous effect
  dy.dx = beta.hat[[marg_var]] + beta.hat[[int.name]] * z
  
  # calculate standard errors for instantaeous effect
  se.dy.dx = sqrt(cov[marg_var, marg_var] +
                     z^2 * cov[int.name, int.name] +
                     2 * z * cov[marg_var, int.name])
  
  # combine into data frame
  data_frame(z = z,
             dy.dx = dy.dx,
             se = se.dy.dx)
}
```

``` {r marginal_age_1, echo=FALSE}
instant_effect(lm_int_biden, "educ") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_hline(yintercept = 0, linetype = 2, color='darkturquoise', size=1) +
  geom_pointrange(color='deeppink') +
  labs(title = "Marginal Effect of Age on Warmth Conditional on Education",
       x = "Education",
       y = "Estimated marginal effect") + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1)) +
  scale_x_continuous(limits = c(0,17), breaks = seq(0,17,by=2))
```

``` {r marginal_age_2, echo=FALSE}
instant_effect(lm_int_biden, "educ") %>%
  ggplot(aes(z, dy.dx)) +
  geom_hline(yintercept = 0, color='darkturquoise', size=1) +
  geom_line(color='deeppink', size=1) +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2, color = 'deeppink', size=1) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2, color = 'deeppink', size=1) +
  labs(title = "Marginal Effect of Age on Warmth Conditional on Education",
       x = "Education",
       y = "Estimated marginal effect") + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1)) +
  scale_x_continuous(limits = c(0,17), breaks = seq(0,17,by=2))
```

We see that the 95% confidence interval for the marginal effect of age conditional on education includes zero for education values ranging from 13 to 16; below 13, it is strictly positive, and above 16, it is strictly negative.  The confidence interval is wide for values at the left tail of the distribution and much narrower for education values in the [12,14] range; this reflects the number of observations with values in those ranges:  There are few respondents with education level below 10, and the interval widens considerably as we move left from there.  There are many respondents with education level ranging from [12,14], so this tightens the interval; it widens as we move to 15 and above.

The plot shows that as education increases from [0,14), age has a positive marginal effect on warmth.  For education = 14, age has no marginal effect on warmth.  As education increases from (14,17], age has a negative marginal effect on warmth.

Next, we test whether the maringal effect of age conditional on education is statistically significant:

``` {r marginal_age_3}
age_marginal = coef(lm_int_biden)[["age"]] + coef(lm_int_biden)[["age:educ"]]

linearHypothesis(lm_int_biden, "age + age:educ")
```

We find that the marginal effect of age conditional on education (`r age_marginal`) is statistically significant at the p<.001 level.

### Marginal effect of education on warmth, conditional on age

``` {r marginal_education_1, echo=FALSE}
instant_effect(lm_int_biden, "age") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_hline(yintercept = 0, linetype = 2, color='darkturquoise', size=1) +
  geom_pointrange(color='deeppink') +
  labs(title = "Marginal Effect of Education on Warmth Conditional on Age",
       x = "Age",
       y = "Estimated marginal effect") + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1)) +
  scale_x_continuous(limits = c(18,93), breaks = seq(20,95,by=10))
```

``` {r marginal_education_2, echo=FALSE}
instant_effect(lm_int_biden, "age") %>%
  ggplot(aes(z, dy.dx)) +
  geom_hline(yintercept = 0, color='darkturquoise', size=1) +
  geom_line(color='deeppink', size=1) +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2, color = 'deeppink', size=1) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2, color = 'deeppink', size=1) +
  labs(title = "Marginal Effect of Education on Warmth Conditional on Age",
       x = "Age",
       y = "Estimated marginal effect") + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1)) +
  scale_x_continuous(limits = c(18,93), breaks = seq(20,95,by=10))
```

We see that the 95% confidence interval is wider at the tails and narrower between [45,65]; this is because there are fewer respondents with ages at the tails of the age distribution and more with ages toward the center of the distribution.  The interval includes zero for ages below 45.  As age increases, the marginal effect of education on warmth conditional on age decreases.

``` {r marginal_education_3}
education_marginal = coef(lm_int_biden)[["educ"]] + coef(lm_int_biden)[["age:educ"]]

linearHypothesis(lm_int_biden, "educ + age:educ")
```

We find that the marginal effect of education on warmth conditional on age (`r education_marginal`) is statistically significant at the p<.05 level.

# Missing data


Estimate the following linear regression model of attitudes towards Joseph Biden:

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3$$

where $Y$ is the Joe Biden feeling thermometer, $X_1$ is age, $X_2$ is gender, and $X_3$ is education. This time, use multiple imputation to account for the missingness in the data. Consider the multivariate normality assumption and transform any variables as you see fit for the imputation stage. Calculate appropriate estimates of the parameters and the standard errors and explain how the results differ from the original, non-imputed model.


``` {r read_data_missing}

df2 = read.csv('biden.csv')

biden_subset_vars = df2 %>%
                    select(biden, age, educ)
```

We begin by visually assessing normality with a scatterplot matrix:

``` {r scatterplot_matrix, warning=FALSE, echo=FALSE}

GGally::ggpairs(biden_subset_vars)
```

The density plots indicate that the continuous variables are not normally distributed.  We continue by assessing normality formally with the Anderson-Darling test:

``` {r a-dar_1}

ad.test(biden_subset_vars$biden)
```

``` {r a-dar_2}

ad.test(biden_subset_vars$age)
```

``` {r a-dar_3}

ad.test(biden_subset_vars$educ)
```

For all three continuous variables, the Anderson-Darling test rejects the null hypothesis that the data are distributed normally.  The test results imply that multivariate normality is out of the question; nevertheless, for the sake of completeness, we continue with formal multivariate normality tests:

``` {r mardia}

mardiaTest(biden_subset_vars)
```

``` {r hz}

hzTest(biden_subset_vars)
```

Both the Mardia and Henze-Zirkler tests indicate that the data are not multivariate normal.

We proceed by transforming the variables in an effort to impose a normal distribution.  First, we take the log; then, we take the square root:

``` {r scatterplot_matrix_log, warning=FALSE, echo=FALSE}

GGally::ggpairs(log(biden_subset_vars + 1))
```

``` {r scatterplot_matrix_sqrt, warning=FALSE, echo=FALSE}

GGally::ggpairs(as.data.frame(biden_subset_vars**(1/2)))
```

We decide that while far from perfect, the best transformations (of the ones we tried) are as follows:

—Biden feeling thermometer:  ln(biden)

—Age:  sqrt(age)

—Education:  sqrt(education)

We transform the variables thus:

``` {r transform_biden}

df2_transformed = df2 %>% mutate(biden = log(biden + 1),
                                 age = sqrt(age),
                                 edu = sqrt(educ))

transformed_subset_vars = df2_transformed %>%
                             select(biden, age, edu)
```

``` {r scatterplot_matrix_transformed, warning=FALSE, echo=FALSE}

GGally::ggpairs(transformed_subset_vars)
```

``` {r mardia2}

mardiaTest(transformed_subset_vars)
```

``` {r hz2}

hzTest(transformed_subset_vars)
```

Despite our efforts, the data are still not multivariate normal.  Nevertheless, we press on.  We estimate three models:

—One with listwise deletion

—A second with full imputation

—A third with imputation based on the transformed variables

``` {r estimate_three_models}

lm_listwise = lm(biden ~ age + female + educ, df2)
amelia_full = amelia(df2, noms = c('female','dem','rep'), m=5, p2s=0)
amelia_transformed = amelia(df2, logs = c('biden'), sqrt = c('age','educ'), noms = c('female','dem','rep'), m=5, p2s=0)

#Code by bsoltoff
models_imp_full = data_frame(data = amelia_full$imputations) %>%
  mutate(model = map(data, ~ lm(biden ~ age +
                                  female + educ,
                                data = .x)),
         coef = map(model, tidy)) %>%
  unnest(coef, .id = "id")
models_imp_full

models_imp_transformed = data_frame(data = amelia_transformed$imputations) %>%
  mutate(model = map(data, ~ lm(biden ~ age +
                                  female + educ,
                                data = .x)),
         coef = map(model, tidy)) %>%
  unnest(coef, .id = "id")
models_imp_transformed
```

``` {r plus_function}
#Code by bsoltoff
mi.meld.plus <- function(df_tidy){
  # transform data into appropriate matrix shape
  coef.out <- df_tidy %>%
    select(id:estimate) %>%
    spread(term, estimate) %>%
    select(-id)
  
  se.out <- df_tidy %>%
    select(id, term, std.error) %>%
    spread(term, std.error) %>%
    select(-id)
  
  combined.results <- mi.meld(q = coef.out, se = se.out)
  
  data_frame(term = colnames(combined.results$q.mi),
             estimate.mi = combined.results$q.mi[1, ],
             std.error.mi = combined.results$se.mi[1, ])
}
```


```{r compare_table, message=FALSE}
tidy(lm_listwise) %>%
  left_join(mi.meld.plus(models_imp_full)) %>%
  select(-statistic, -p.value)
```

```{r compare_table2, message=FALSE}
tidy(lm_listwise) %>%
  left_join(mi.meld.plus(models_imp_transformed)) %>%
  select(-statistic, -p.value)
```


``` {r compare_viz}
#Code by bsoltoff
bind_rows(orig = tidy(lm_listwise),
          full_imp = mi.meld.plus(models_imp_full) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          trans_imp = mi.meld.plus(models_imp_transformed) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          .id = "method") %>%
  mutate(method = factor(method, levels = c("orig", "full_imp", "trans_imp"),
                         labels = c("Listwise deletion", "Full imputation",
                                    "Transformed imputation")),
         term = factor(term, levels = c("(Intercept)", "age",
                                        "female", "educ"),
                       labels = c("Intercept", "sqrt(Age)", "Female",
                                  "sqrt(Education)"))) %>%
  filter(term != "Intercept") %>%
  ggplot(aes(fct_rev(term), estimate, color = fct_rev(method),
             ymin = estimate - 1.96 * std.error,
             ymax = estimate + 1.96 * std.error)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_pointrange(position = position_dodge(.75)) +
  coord_flip() +
  scale_color_manual(guide = guide_legend(reverse = TRUE), values=c('deeppink','darkturquoise','orange')) +
  labs(title = "Comparing Regression Results",
       subtitle = "Omitting Intercept from Plot",
       x = NULL,
       y = "Estimated parameter",
       color = NULL) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))
```

By comparing the parameter estimates across the different methods, we can see that the confidence intervals for the parameters overlap.  This suggests that there may be little substantive or statistically significant difference among methods.  Notably, the transformed parameters for female and education aremuch wider than those for listwise deletion or full imputation.
